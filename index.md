---
layout: default
---

# Overview

| **Date** |  |
| **Location** | The workshop will be held *virtually*. |


# Background
While machine learning (ML) models have achieved great success in many applications, concerns have been raised about their potential vulnerabilities and risks when applied to safety-critical applications. On the one hand, from the security perspective, studies have been conducted to explore worst-case attacks against ML models and therefore inspire both empirical and certifiable defense approaches. On the other hand, from the safety perspective, researchers have looked into safe constraints, which should be satisfied by safe AI systems (e.g. autonomous driving vehicles should not hit pedestrians). This workshop makes the first attempts towards bridging the gap of these two communities and aims to discuss principles of developing secure and safe ML systems. The workshop also focuses on how future practitioners should prepare themselves for reducing the risks of unintended behaviors of sophisticated ML models. 

The workshop will bring together experts from machine learning, computer security, and AI safety communities. We attempt to highlight recent related work from different communities, clarify the foundations of secure and safe ML, and chart out important directions for future work and cross-community collaborations.

There have been some previous workshops on the security and privacy perspectives of machine learning, as well as the formal verification for machine learning models. Different from previous workshops, our proposed workshop seeks to broaden the community by appealing to researchers working on both theoretical studies and practical applications, including theoretical machine learning and real-world deployment of machine learning models (e.g., autonomous vehicles and robotics). In particular, one of our goals is to bring solutions from the adversarial robustness literature to bear on the robustness of AI systems that often fail due to the distribution shift, sensor malfunctions, and unexpected inputs.  Likewise, we seek to introduce the techniques to improve safety under uncertainty from the ML systems/applications community, which provides a different angle from the study of robustness from the security perspective.

Topics include but are not limited to:

* Adversarial attacks against ML systems, including training and test time attacks
* Improving model robustness against attacks, including empirical and certifiable defenses
* Theoretical understanding of adversarial machine learning
* AI safety for real-world deployment
* Formal verification of machine learning systems
* Explainable and interpretable AI
* Futuristic concerns about AI safety
